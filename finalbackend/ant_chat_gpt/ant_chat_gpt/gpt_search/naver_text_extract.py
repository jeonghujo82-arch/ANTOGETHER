from .naver_crawler import NaverCrawler
from openai import OpenAI
from datetime import datetime
from dotenv import load_dotenv
import os
import json
import re

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
# Environment variables are now loaded globally in backend_new.py
print(f"DEBUG: OPENAI_API_KEY from .env: {os.getenv('OPENAI_API_KEY')}")
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

class NewsScheduleExtractor:
    """
    ê¸°ì‚¬ ë³¸ë¬¸ë“¤ë¡œë¶€í„° ì¼ì • ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤
    """
    def __init__(self, model="gpt-4o-mini"):
        self.model = model
        self.client = client

    def _clean_json_output(self, text):
        return re.sub(r"^```json|```$", "", text.strip()).strip()

    def extract_from_texts(self, page_texts):
        today = datetime.now().strftime("%Y-%m-%d")
        content = "nn".join(page_texts)
        prompt = f'''
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ë‚´ìš©ì—ì„œ ì¼ì • ì •ë³´ë¥¼ ì¶”ì¶œí•´ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:

'~ë¶€í„°', '~ê¹Œì§€' ë“±ì˜ í‘œí˜„ì€ start, endë¡œ ë¶„ë¦¬í•˜ì„¸ìš”.
'~ì˜ˆì •', '~í•˜ì' í¬í•¨ ë¬¸ì¥ì€ eventë¡œ ê°„ì£¼í•˜ì„¸ìš”.
startì™€ endëŠ” ë™ì¼í•´ë„ í—ˆìš©ë©ë‹ˆë‹¤.

{content}

ì˜ˆì‹œ í˜•ì‹:
{{
  "events": [
    {{
      "start_date": "YYYY-MM-DD-HH:mm",
      "end_date": "YYYY-MM-DD-HH:mm",
      "title": "ì¼ì • ë‚´ìš©"
    }}
  ]
}}
'''
        try:
            res = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": f"ë„ˆëŠ” ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì¼ì • ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” AIì•¼. í˜„ì¬ ë‚ ì§œ:{today}, í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê³¼ê±°ì˜ ì¼ì •ì€ ì¶”ì¶œí•˜ì§€ë§ˆ."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2
            )
            raw_output = res.choices[0].message.content
            cleaned = self._clean_json_output(raw_output)

            # âœ… ì¶”ì¶œëœ GPT ì‘ë‹µ í™•ì¸ìš© ì¶œë ¥
            print("nğŸ“‹ GPT ì‘ë‹µ ì›ë³¸:n", raw_output[:300], "...")  # ì¼ë¶€ë§Œ ì¶œë ¥
            print("ğŸ“‹ ì •ì œëœ JSON ë°ì´í„°:n", cleaned)

            return json.loads(cleaned)

        except json.JSONDecodeError:
            print("âŒ GPT ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨:n", cleaned)
            return {"events": []}
        except Exception as e:
            print("âŒ ì¼ì • ì¶”ì¶œ ì‹¤íŒ¨:", e)
            return {"events": []}

    def extract_search_query(self, user_input):
        """
        ì‚¬ìš©ìì˜ ë¬¸ì¥ì—ì„œ ì›¹ ê²€ìƒ‰ìš© í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
        """
        prompt = f"""
ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ì›¹ ê²€ìƒ‰ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•µì‹¬ í‚¤ì›Œë“œë¥¼ ê°„ê²°í•˜ê²Œ ë½‘ì•„ì¤˜.
ë‚ ì§œ, 'ì¼ì •', 'ì•Œë ¤ì¤˜' ê°™ì€ ì¼ë°˜ì ì¸ í‘œí˜„ì€ ì œì™¸í•˜ê³ , í•µì‹¬ ì£¼ì œë§Œ ë‚¨ê²¨ì¤˜.

ì˜ˆì‹œ:
ì…ë ¥: "í”Œë ˆì´ë¸Œ ì„œìš¸ ì½˜ì„œíŠ¸ ì¼ì • ì•Œë ¤ì¤˜"
ì¶œë ¥: "í”Œë ˆì´ë¸Œ ì„œìš¸ ì½˜ì„œíŠ¸"
ì…ë ¥: "ë‰´ì§„ìŠ¤ ì»´ë°± ë‚ ì§œ ì•Œë ¤ì¤˜"
ì¶œë ¥: "ë‰´ì§„ìŠ¤ ì»´ë°±"

ì…ë ¥: "{user_input}"
ì¶œë ¥:
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ë„ˆëŠ” ë¬¸ì¥ì—ì„œ í•µì‹¬ ê²€ìƒ‰ì–´ë§Œ ë½‘ì•„ì£¼ëŠ” AIì•¼."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2
            )
            keyword = response.choices[0].message.content.strip().strip('"')
            print(f"ğŸ” ì¶”ì¶œëœ ê²€ìƒ‰ì–´: {keyword}")  # âœ… í‚¤ì›Œë“œ ì¶œë ¥
            return keyword
        except Exception as e:
            print(f"âŒ OpenAI í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return "ê²€ìƒ‰ì–´ ì¶”ì¶œ ì‹¤íŒ¨"
        
    def extraction(self, query: str = None):
        """
        ë‰´ìŠ¤ ê²€ìƒ‰ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë„¤ì´ë²„ì—ì„œ ê¸°ì‚¬ë“¤ì„ ê²€ìƒ‰í•˜ê³ ,
        ê¸°ì‚¬ ë³¸ë¬¸ì„ GPTì—ê²Œ ë„˜ê²¨ì„œ ì¼ì • ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤ ë©”ì„œë“œ.

        :param query: ì§ì ‘ ì „ë‹¬í•  ê²€ìƒ‰ì–´ (Noneì´ë©´ ì‚¬ìš©ì ì…ë ¥ ë°›ìŒ)
        :return: ì¼ì • dict (e.g. {"events": [...]})
        """
        if query is None:
            query = input("ğŸ” ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")

        print(f"nğŸŸ£ ì‚¬ìš©ì ì…ë ¥: {query}")
        extracted_query = self.extract_search_query(query)

        crawler = NaverCrawler()
        search_results = crawler.search(extracted_query, display=5)  # âœ… ë‰´ìŠ¤ ê°œìˆ˜ 5ê°œë¡œ ì¦ê°€

        if not search_results:
            print("âŒ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return {"events": []}

        page_texts = []
        for item in search_results:
            print(f"nğŸ“° ê¸°ì‚¬ ì œëª©: {item['title']}")
            text = crawler.extract_text(item["link"])
            print(f"ğŸ“„ ë³¸ë¬¸ ê¸¸ì´: {len(text)}, ì•ë¶€ë¶„: {text[:100]}...")  # âœ… ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°
            page_texts.append(text)

        result = self.extract_from_texts(page_texts)

        print("nğŸ“… ì¶”ì¶œëœ ì¼ì • JSON:n", result)

        return result
